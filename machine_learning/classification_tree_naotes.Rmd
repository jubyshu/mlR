---
title: "Classification Trees Notes"
author: "jubyr"
date: "6/12/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Build a classification tree

Let's get started and build our first classification tree. A classification tree is a decision tree that performs a classification (vs regression) task.

You will train a decision tree model to understand which loan applications are at higher risk of default using a subset of the German Credit Dataset. The response variable, called "default", indicates whether the loan went into a default or not, which means this is a binary classification problem (there are just two classes).

You will use the `rpart` package to fit the decision tree and the `rpart.plot` package to visualize the tree.

```{r}
library(rpart)
library(rpart.plot)
```

First, take a look at the data.

```{r}
creditsub <- read.csv("creditsub.csv")
str(creditsub)
```

Fit the classification decision tree using the `rpart()` function from the rpart package. In the `rpart()` function, note that you'll also have to provide the training data frame.

```{r}
# Create the model
credit_model <- rpart(formula = default ~ ., 
                      data = creditsub, 
                      method = "class")
```

Using the model object that you create, plot the decision tree model using the `rpart.plot()` function from the `rpart.plot` package.

```{r}
# Display the results
rpart.plot(x = credit_model, yesno = 2, type = 0, extra = 0)
```

## Train/test split

For this exercise, you'll randomly split the German Credit Dataset into two pieces: a training set (80%) called `credit_train` and a test set (20%) that we will call `credit_test`.

```{r}
# Total number of rows in the credit data frame
n <- nrow(creditsub)

# Number of rows for the training set (80% of the dataset)
n_train <- round(0.8 * n) 

# Create a vector of indices which is an 80% random sample
set.seed(123)
train_indices <- sample(1:n, n_train)

# Subset the credit data frame to training indices only
credit_train <- creditsub[train_indices, ]  
  
# Exclude the training indices to create the test set
credit_test <- creditsub[-train_indices, ]  
```

## Train a classification tree model

```{r}
# Train the model (to predict 'default')
credit_model <- rpart(formula = default ~ ., 
                      data = credit_train, 
                      method = "class")

# Look at the model output                      
print(credit_model)
```

## Compute confusion matrix

As discussed in the previous video, there are a number of different metrics by which you can measure the performance of a classification model. In this exercise, we will evaluate the performance of the model using test set classification error. A confusion matrix is a convenient way to examine the per-class error rates for all classes at once.

The `confusionMatrix()` function from the `caret` package prints both the confusion matrix and a number of other useful classification metrics such as "Accuracy" (fraction of correctly classified instances).

```{r}
library(caret)
```


```{r}
# Generate predicted classes using the model object
class_prediction <- predict(object = credit_model,  
                        newdata = credit_test,   
                        type = "class")  
                            
# Calculate the confusion matrix for the test set
confusionMatrix(data = class_prediction,       
                reference = credit_test$default)  
```

## Compare models with a different splitting criterion

Train two models that use a different splitting criterion and use the validation set to choose a "best" model from this group. To do this you'll use the `parms` argument of the `rpart()` function. This argument takes a named list that contains values of different parameters you can use to change how the model is trained. Set the parameter `split` to control the splitting criterion.

Classification error is the fraction of incorrectly classified instances. Compute and compare the test set classification error of the two models by using the `ce()` function.

```{r}
library(Metrics)
# Train a gini-based model
credit_model1 <- rpart(formula = default ~ ., 
                       data = credit_train, 
                       method = "class",
                       parms = list(split = "gini"))

# Train an information-based model
credit_model2 <- rpart(formula = default ~ ., 
                       data = credit_train, 
                       method = "class",
                       parms = list(split = "information"))

# Generate predictions on the validation set using the gini model
pred1 <- predict(object = credit_model1, 
             newdata = credit_test,
             type = "class")    

# Generate predictions on the validation set using the information model
pred2 <- predict(object = credit_model2, 
             newdata = credit_test,
             type = "class")

# Compare classification error
ce(actual = credit_test$default, 
   predicted = pred1)
ce(actual = credit_test$default, 
   predicted = pred2)  
```